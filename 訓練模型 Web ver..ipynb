{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "drive_path = 'C:/AIJAVAProject/沛綸小組_2021/model/'\n",
    "if not os.path.isdir(drive_path):\n",
    "    os.makedirs(drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import sys\n",
    "sys.path.append('C:/AIJAVAProject/沛綸小組_2021/local_training')\n",
    "sys.path.append('C:/AIJAVAProject/沛綸小組_2021/local_training/keras_position_wise_feed_forward')\n",
    "sys.path.append('C:/AIJAVAProject/沛綸小組_2021/local_training/tensorflow_fast_attention')\n",
    "sys.path.append('C:/AIJAVAProject/沛綸小組_2021/local_training/keras_performer')\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performer_ver_4 as pfr\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import DataGeneratorTrain as DGTrain\n",
    "import DataGeneratorValidation as DGValidation\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDictionary(dt, file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"wb\")\n",
    "    pickle.dump(dt, a_file)\n",
    "    a_file.close()\n",
    "\n",
    "def loadDictionary(file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"rb\")\n",
    "    dt = pickle.load(a_file)\n",
    "    return dt\n",
    "\n",
    "def load(model_name):\n",
    "    from keras_performer import performer_ver_4 #需要更正是哪一版的performer\n",
    "    from tensorflow import keras\n",
    "    from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "    from keras_pos_embd import TrigPosEmbedding\n",
    "    from tensorflow_fast_attention.fast_attention import softmax_kernel_transformation , Attention, SelfAttention #新增softmax_kernel_transformation\n",
    "    from keras_position_wise_feed_forward.feed_forward import FeedForward \n",
    "\n",
    "    co = performer_ver_4.get_custom_objects()  #需要更正是哪一版的performer\n",
    "    co[\"softmax_kernel_transformation\"] = softmax_kernel_transformation #新增softmax_kernel_transformation\n",
    "    \n",
    "    model = keras.models.load_model(model_name, custom_objects= co)\n",
    "    s = loadDictionary(drive_path + 'source_token_dict.pickle')\n",
    "    t = loadDictionary(drive_path + 'target_token_dict.pickle')\n",
    "    t_inv = loadDictionary(drive_path + 'target_token_dict_inv.pickle')\n",
    "        \n",
    "def loadMaxLen(filename):     \n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "    \n",
    "def plotTrainingLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right') \n",
    "    plt.show()\n",
    "    \n",
    "def plotTrainingEFFOLoss(history):\n",
    "    plt.plot(history.history['classifier_decoder_model_loss'])\n",
    "    plt.plot(history.history['val_classifier_decoder_model_loss'])\n",
    "    plt.title('model error_feed_forward_output1 loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right') \n",
    "    plt.show()\n",
    "    \n",
    "def plotTrainingDOLoss(history):\n",
    "    plt.plot(history.history['classifier_decoder_model_1_loss'])\n",
    "    plt.plot(history.history['val_classifier_decoder_model_1_loss'])\n",
    "    plt.title('model Decoder-Output loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right') \n",
    "    plt.show()\n",
    "    \n",
    "def plotTrainingEFFOBinAcc(history):\n",
    "    plt.plot(history.history['classifier_decoder_model_binary_accuracy'])\n",
    "    plt.plot(history.history['val_classifier_decoder_model_binary_accuracy'])\n",
    "    plt.title('model error_feed_forward_output1 binary_accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy'], loc='upper left') \n",
    "    plt.show()\n",
    "    \n",
    "def plotTrainingDOAcc(history):\n",
    "    plt.plot(history.history['classifier_decoder_model_1_sparse_categorical_accuracy'])\n",
    "    plt.plot(history.history['val_classifier_decoder_model_1_sparse_categorical_accuracy'])\n",
    "    plt.title('model Decoder-Output accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy'], loc='upper left') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "        source_token_dict = loadDictionary(drive_path + 'source_token_dict.pickle')\n",
    "        target_token_dict = loadDictionary(drive_path + 'target_token_dict.pickle')\n",
    "        target_token_dict_inv = loadDictionary(drive_path + 'target_token_dict_inv.pickle')\n",
    "        \n",
    "        print(\"source_token_dict len: \", len(source_token_dict))\n",
    "        print(\"target_token_dict len: \", len(target_token_dict))\n",
    "        print(\"target_token_dict_inv len: \", len(target_token_dict_inv))\n",
    "        \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        source_max_len = int(source_max_len_loaded[0])\n",
    "        \n",
    "        target_max_len_loaded = loadMaxLen(drive_path + \"target_max_len.txt\")\n",
    "        target_max_len = int(target_max_len_loaded[0])\n",
    "        \n",
    "        max_seq_len=source_max_len #(source_max_len, target_max_len)\n",
    "            \n",
    "        # Build & fit model      \n",
    "        model = pfr.get_model(\n",
    "            max_input_len=(source_max_len, target_max_len), \n",
    "            token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "            embed_dim=32,\n",
    "            encoder_num=6,\n",
    "            decoder_num=6,\n",
    "            head_num=4,\n",
    "            hidden_dim=128,\n",
    "            dropout_rate=0.05,\n",
    "            use_same_embed=False  # Use different embeddings for different languages\n",
    "        )\n",
    "\n",
    "        losses = {\n",
    "                \"classifier_decoder_model\": \"binary_crossentropy\",\n",
    "                \"classifier_decoder_model_1\": \"sparse_categorical_crossentropy\",\n",
    "        }\n",
    "        lossWeights = {\"classifier_decoder_model\": 1.0, \"classifier_decoder_model_1\": 1.0}\n",
    "        metrics = {\"classifier_decoder_model\": tf.keras.metrics.BinaryAccuracy(), \"classifier_decoder_model_1\": tf.keras.metrics.SparseCategoricalAccuracy()}\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=losses, loss_weights=lossWeights, metrics=metrics)     \n",
    "        model.summary()\n",
    "        \n",
    "        #for output \n",
    "        output_buffer_params = { \n",
    "            \"data_path\": [\"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\", \"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\"],\n",
    "            \"data_number\":[11011, 11011],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [11011, 11011]\n",
    "            }\n",
    "        #for input\n",
    "        input_buffer_params = {\n",
    "            \"data_path\": [\"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\", \"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\"],\n",
    "            \"data_number\":[11011, 11011],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [11011, 11011]\n",
    "            }\n",
    "        \n",
    "        #for output \n",
    "        validation_output_buffer_params = { \n",
    "            \"data_path\": [\"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\", \"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\"],\n",
    "            \"data_number\":[2753, 2753],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [2753, 2753]\n",
    "            }\n",
    "        #for input\n",
    "        validation_input_buffer_params = {\n",
    "            \"data_path\": [\"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\", \"C:\\\\AIJAVAProject\\\\沛綸小組_2021\\\\model\\\\\"],\n",
    "            \"data_number\":[2753, 2753],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [2753, 2753]\n",
    "            }\n",
    "        # Generators\n",
    "        training_generator = DGTrain.DataGeneratorTrain(input_buffer_params, output_buffer_params,\n",
    "                                               [list(range(11011)), list(range(11011))],\n",
    "                                               max_text_len=max_seq_len,\n",
    "                                              )\n",
    "        \n",
    "        validation_generator = DGValidation.DataGeneratorValidation(validation_input_buffer_params, validation_output_buffer_params,\n",
    "                                               [list(range(2753)), list(range(2753))],\n",
    "                                               max_text_len=max_seq_len,\n",
    "                                              )\n",
    "        \n",
    "        # Train model on dataset\n",
    "        EPOCHS = \"1000\"\n",
    "        \n",
    "        model_checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=drive_path+\"checkpoint_\" + EPOCHS + \".h5\",\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "            \n",
    "        history = model.fit_generator( \n",
    "            generator=training_generator,        \n",
    "            epochs=int(EPOCHS),    \n",
    "            verbose=2,\n",
    "            validation_data=validation_generator,\n",
    "            callbacks=[model_checkpoint_callback],\n",
    "        )\n",
    "        saveDictionary(history.history, drive_path + \"history_\" + EPOCHS)\n",
    "        np.save(drive_path + \"history_\" + EPOCHS + \"_npy.npy\", history.history)\n",
    "        model.save(drive_path + \"test_model_\" + EPOCHS + \".h5\")\n",
    "        \n",
    "        #print(history.history.keys()) \n",
    "        plotTrainingLoss(history)\n",
    "        plotTrainingEFFOLoss(history)\n",
    "        plotTrainingDOLoss(history)\n",
    "\n",
    "        plotTrainingEFFOBinAcc(history)\n",
    "        plotTrainingDOAcc(history)\n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
