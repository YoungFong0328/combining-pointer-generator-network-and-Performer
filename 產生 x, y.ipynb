{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "drive_path = 'D:/model/'\n",
    "if not os.path.isdir(drive_path):\n",
    "    os.makedirs(drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import unittest\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length\n",
    "\n",
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length\n",
    "        \n",
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'NUM_INT', '>'], [\"<NUM_INT>\"])\n",
    "    replace_sublist(x, ['<', 'NUM_FLOAT', '>'], [\"<NUM_FLOAT>\"])\n",
    "    replace_sublist(x, ['<', 'STRING', '>'], [\"<STRING>\"])\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x\n",
    "\n",
    "def parseSentence(x):\n",
    "    tokenizer =  RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        elif (ord(x[i])>=48 and ord(x[i])<=57):\n",
    "            inp=\"D\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])                \n",
    "            \n",
    "        elif state==\"ASCII\":\t\n",
    "            if inp==\"D\" or inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs) #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"NUMBER\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\t\t\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs) #nltk.word_tokenize(chrs)\n",
    "    return replaceTAGS(tokens)\n",
    "\n",
    "def readcode(fname):\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        return data\n",
    "\n",
    "def outputsplit(txt): #txt: \"<BOTM><BOT>32<EOT><BOM>XXX<EOM><EOTM>....\"\n",
    "    pattern = re.compile(r'<BOTM>(.*?)<EOTM>')\n",
    "    x = re.findall(r\"<BOTM> *<BOT>(.*?)<EOT>(.*?)<EOTM>\", txt, re.DOTALL)\n",
    "    y=list(zip((*x)))#[('32', ...), ('<BOM>XXX<EOM>', ...)]\n",
    "    err_codes = [int(code) for code in y[0]]\n",
    "    return err_codes, y[1] #erro int codes, messages\n",
    "    \n",
    "def saveMaxLen(filename, data): \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(str(data))\n",
    "        f.close()\n",
    "\n",
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    #print(len(data))\n",
    "    #print(data[0].shape)\n",
    "    #print(data[1].shape)\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)\n",
    "\n",
    "def saveDictionary(dt, file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"wb\")\n",
    "    pickle.dump(dt, a_file)\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<BOC>': 1,\n",
    "            '<EOC>': 2,\n",
    "            '<CR>': 3,\n",
    "            '<NUM_INT>': 4,\n",
    "            '<NUM_FLOAT>': 5,\n",
    "            '<STRING>': 6,\n",
    "        }\n",
    "        \n",
    "        self.target_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOM>': 3,\n",
    "            '<EOM>': 4,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "\n",
    "    def test_translate(self):\n",
    "        Input_Path = \"D:\\\\Proportional Augmentation (替換字串與數字)\\\\Input\\\\**\\\\*.txt\"\n",
    "        Output_Path = \"D:\\\\Proportional Augmentation (替換字串與數字)\\\\Output\\\\**\\\\*.txt\"\n",
    "        \n",
    "        #Input_Path = \"D:\\\\Augmentation\\\\Input\\\\**\\\\*.txt\"\n",
    "        #Output_Path = \"D:\\\\Augmentation\\\\Output\\\\**\\\\*.txt\"\n",
    "        \n",
    "        in_path = sorted(glob.glob(Input_Path))\n",
    "        out_path = sorted(glob.glob(Output_Path))\n",
    "         \n",
    "        source_max_len = 0\n",
    "        target_max_len = 0\n",
    "        data_num = 17640\n",
    "        block_num = 17640\n",
    "        for loop in range(0, math.ceil(data_num/block_num)):\n",
    "            source_tokens = []\n",
    "            target_errors=[]\n",
    "            target_tokens = []     \n",
    "            if data_num % block_num == 0: \n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < data_num // block_num else data_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            \n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])\n",
    "                \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "        \n",
    "            for i in range(dirs):\n",
    "                Output_fullpath.append(out_path[loop*block_num + i])\n",
    "\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):  \n",
    "                    o1, o2 = outputsplit(readcode(f))#o1: list of error codes\n",
    "                    o2 = \"\".join(o2)\n",
    "                    ps = parseSentence(o2) ##<-----parse messages\n",
    "                    target_errors.append(o1)\n",
    "                    target_tokens.append(ps)\n",
    "            \n",
    "            source_tokens2 = []\n",
    "            target_errors2 = []\n",
    "            target_tokens2 = []\n",
    "\n",
    "            THRESHOLD_FILE_LEN = 1000\n",
    "\n",
    "            for i in range(len(source_tokens)):\n",
    "              src = source_tokens[i]\n",
    "              target_error = target_errors[i]\n",
    "              target = target_tokens[i]\n",
    "              if (len(src)<=THRESHOLD_FILE_LEN and  len(target)<=THRESHOLD_FILE_LEN):\n",
    "                source_tokens2.append(src)\n",
    "                target_errors2.append(target_error)\n",
    "                target_tokens2.append(target)\n",
    "            source_tokens = source_tokens2\n",
    "            target_errors = target_errors2 #list of intgers, error types\n",
    "            target_tokens = target_tokens2\n",
    "     \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            self._build_token_dict(self.target_token_dict, target_tokens)\n",
    "            target_token_dict_inv = {v: k for k, v in self.target_token_dict.items()}\n",
    "\n",
    "            # Add special tokens\n",
    "            encode_tokens = [tokens for tokens in source_tokens]\n",
    "            decode_tokens = [['<START>'] + tokens for tokens in target_tokens]\n",
    "            output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "                           \n",
    "            sl = max(map(len, encode_tokens))\n",
    "            tl = max(map(len, decode_tokens))\n",
    "            source_max_len = max(sl, tl, source_max_len)\n",
    "            saveMaxLen(drive_path + \"source_max_len.txt\", source_max_len)\n",
    "            target_max_len = max(sl, tl, target_max_len)\n",
    "            saveMaxLen(drive_path + \"target_max_len.txt\", target_max_len)\n",
    "         \n",
    "        print(\"source_max_len:\", source_max_len)\n",
    "        print(\"target_max_len:\", target_max_len)\n",
    "        \n",
    "        #ready to pad and save data\n",
    "        for loop in range(0, math.ceil(data_num/block_num)):\n",
    "            print(\"loop:\", loop)\n",
    "            source_tokens = []\n",
    "            target_errors=[]\n",
    "            target_tokens = []  \n",
    "            if data_num % block_num == 0: \n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < data_num // block_num else data_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            \n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])\n",
    "                \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "        \n",
    "            for i in range(dirs):\n",
    "                Output_fullpath.append(out_path[loop*block_num + i])\n",
    "\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):  \n",
    "                    o1, o2 = outputsplit(readcode(f))#o1: list of error codes\n",
    "                    o2 = \"\".join(o2)\n",
    "                    ps = parseSentence(o2) ##<-----parse messages\n",
    "                    target_errors.append(o1)\n",
    "                    target_tokens.append(ps)\n",
    "            \n",
    "            print(\"XXXX: \" , len(source_tokens))\n",
    "            print(\"YYYY: \" , len(target_errors))\n",
    "            print(\"ZZZZ: \" , len(target_tokens))\n",
    "            \n",
    "            source_tokens2 = []\n",
    "            target_errors2 = []\n",
    "            target_tokens2 = []\n",
    "\n",
    "            THRESHOLD_FILE_LEN = 1000\n",
    "\n",
    "            for i in range(len(source_tokens)):\n",
    "              src = source_tokens[i]\n",
    "              target_error = target_errors[i]\n",
    "              target = target_tokens[i]\n",
    "              if (len(src)<=THRESHOLD_FILE_LEN and len(target)<=THRESHOLD_FILE_LEN):\n",
    "                source_tokens2.append(src)\n",
    "                target_errors2.append(target_error)\n",
    "                target_tokens2.append(target)\n",
    "            source_tokens = source_tokens2\n",
    "            target_errors = target_errors2 #list of intgers, error types\n",
    "            target_tokens = target_tokens2\n",
    "\n",
    "            print(\"XXXX2: \" , len(source_tokens)) #262 files\n",
    "            print(\"YYYY2: \" , len(target_errors)) #262 answers           \n",
    "            print(\"ZZZZ2: \" , len(target_tokens)) #262 answers           \n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            self._build_token_dict(self.target_token_dict, target_tokens)\n",
    "            target_token_dict_inv = {v: k for k, v in self.target_token_dict.items()}\n",
    "\n",
    "            # Add special tokens\n",
    "            encode_tokens = [tokens for tokens in source_tokens]\n",
    "            decode_tokens = [['<START>'] + tokens for tokens in target_tokens]\n",
    "            output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "                    \n",
    "            # Padding\n",
    "            encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "            decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "            output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "            encode_input = [list(map(lambda x: self.source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "            decode_input = [list(map(lambda x: self.target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "            decode_output2 = [list(map(lambda x: [self.target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
    "            \n",
    "            print(\"source_token_dict len: \", len(self.source_token_dict))\n",
    "            print(\"target_token_dict len: \", len(self.target_token_dict))\n",
    "            print(\"target_token_dict_inv len: \", len(target_token_dict_inv))\n",
    "            \n",
    "            saveDictionary(encode_tokens, drive_path + 'source_token_dict.pickle')\n",
    "            saveDictionary(self.target_token_dict, drive_path + 'target_token_dict.pickle')\n",
    "            saveDictionary(target_token_dict_inv, drive_path + 'target_token_dict_inv.pickle')  \n",
    "            \n",
    "            saveDictionary(self.source_token_dict, drive_path + 'source_token_dict.pickle')\n",
    "            saveDictionary(self.target_token_dict, drive_path + 'target_token_dict.pickle')\n",
    "            saveDictionary(target_token_dict_inv, drive_path + 'target_token_dict_inv.pickle')   \n",
    "\n",
    "            #print(\"encode_input\", np.asarray(encode_input).shape) #(271, 798)\n",
    "            #print(\"decode_input\", np.asarray(decode_input).shape) #(271, 798)\n",
    "            #print(\"decode_output2\",  np.asarray(decode_output2).shape) #(271, 798, 1)\n",
    "            #target errors: into 0/1 arrays from target_errors\n",
    "            decode_output1 =[ [0]*36 for i in range(len(target_errors))]\n",
    "            for i in range(len(target_errors)):\n",
    "                    codes= target_errors[i]\n",
    "                    for code in codes:  \n",
    "                            decode_output1[i][code-1] = 1\n",
    "            #print(decode_output1)\n",
    "\n",
    "            x=list(zip(np.array(encode_input), np.array(decode_input)))\n",
    "            y=list(zip(np.array(decode_output1), np.array(decode_output2))) #np.array(decode_output2)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            x_test = list(zip(*x_test))\n",
    "            x_test[0] = np.array(x_test[0])\n",
    "            x_test[1] = np.array(x_test[1])\n",
    "\n",
    "            y_test = list(zip(*y_test))\n",
    "            y_test[0] = np.array(y_test[0]) #decode_output1\n",
    "            y_test[1] = np.array(y_test[1]) #decode_output2\n",
    "            #print(y_test[0].shape)\n",
    "            #print(y_test[1].shape)\n",
    "\n",
    "            #x=[np.array(encode_input * 1), np.array(decode_input * 1)] #(2, 271, 798)\n",
    "            #y=np.array(decode_output2 * 1) #(271, 798, 1)     \n",
    "            \n",
    "            # x_train, x_test: [array, array ]\n",
    "            # y_train, y_test: array     \n",
    "            \n",
    "            x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "            x_train = list(zip(*x_train))\n",
    "            x_train[0] = np.asarray(x_train[0]) #encode_input\n",
    "            x_train[1] = np.asarray(x_train[1]) #decode_input\n",
    "            print(x_train[0].shape)\n",
    "            print(x_train[1].shape)\n",
    "\n",
    "            y_train = list(zip(*y_train))\n",
    "            y_train[0] = np.asarray(y_train[0]) #decode_output1\n",
    "            y_train[1] = np.asarray(y_train[1]) #decode_output2\n",
    "            print(y_train[0].shape)\n",
    "            print(y_train[1].shape)\n",
    "            \n",
    "            x_validation = list(zip(*x_validation))\n",
    "            x_validation[0] = np.asarray(x_validation[0])\n",
    "            x_validation[1] = np.asarray(x_validation[1])\n",
    "\n",
    "            y_validation = list(zip(*y_validation))\n",
    "            y_validation[0] = np.asarray(y_validation[0]) #decode_output1\n",
    "            y_validation[1] = np.asarray(y_validation[1]) #decode_output2\n",
    "            \n",
    "            print(\"x_train[0] shape:\", x_train[0].shape)\n",
    "            print(\"x_validation[0] shape:\", x_validation[0].shape)\n",
    "            print(\"x_test[0] shape:\", x_test[0].shape)\n",
    "            \n",
    "            saveTestTrainData(drive_path + \"x_train[0]_\" + str(loop) + \".npy\", x_train[0])\n",
    "            saveTestTrainData(drive_path + \"x_train[1]_\" + str(loop) + \".npy\", x_train[1])\n",
    "            saveTestTrainData(drive_path + \"x_test[0]_\" + str(loop) + \".npy\", x_test[0])\n",
    "            saveTestTrainData(drive_path + \"x_test[1]_\" + str(loop) + \".npy\", x_test[1])\n",
    "            saveTestTrainData(drive_path + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(drive_path + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            saveTestTrainData(drive_path + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(drive_path + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "            saveTestTrainData(drive_path + \"x_validation[0]_\" + str(loop) + \".npy\", x_validation[0])\n",
    "            saveTestTrainData(drive_path + \"x_validation[1]_\" + str(loop) + \".npy\", x_validation[1])\n",
    "            saveTestTrainData(drive_path + \"y_validation[0]_\" + str(loop) + \".npy\", y_validation[0])\n",
    "            saveTestTrainData(drive_path + \"y_validation[1]_\" + str(loop) + \".npy\", y_validation[1])\n",
    "            \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
